{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f8543e",
   "metadata": {},
   "source": [
    "# Carga Gold - Fato Carteira\n",
    "\n",
    "Este notebook realiza a carga da fato de cota√ß√£o (fato_cotacao) a partir dos dados da tabela staging de controle ativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31477c71",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd7cfa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_config import init_spark\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee3a7e",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8dbd93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Spark 3.5.7 iniciado com Hive local persistente!\n",
      "üìÅ Warehouse: D:/Projetos/DataLake/spark-warehouse\n",
      "üìÅ Metastore: D:/Projetos/DataLake/metastore_db\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = init_spark(\"Carga fato carteira\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d46f1",
   "metadata": {},
   "source": [
    "## Define Delta Table Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea17242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define caminhos locais onde ser√£o armazenadas as tabelas Delta\n",
    "base_silver_path = \"D:/Projetos/Jornada_financas_pessoais/data/delta/silver\"\n",
    "base_gold_path = \"D:/Projetos/Jornada_financas_pessoais/data/delta/gold\"\n",
    "\n",
    "# Define caminhos das tabelas Delta\n",
    "delta_path_controle_ativo = f\"{base_silver_path}/stg_controle_ativo\"\n",
    "delta_path_fato_carteira = f\"{base_gold_path}/fato_carteira\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b7051",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c603623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pipeline para carregar posi√ß√£o mensal de carteira de investimentos\n",
    "Abordagem funcional\n",
    "\"\"\"\n",
    "def extrair_operacoes_mes(spark: SparkSession, mes_ref_base: str, cpf: str = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extrair opera√ß√µes do m√™s da tabela silver\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        mes_ref_base: M√™s de refer√™ncia base no formato 'YYYY-MM'\n",
    "        cpf: CPF do investidor (opcional)\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        mes_referencia,\n",
    "        '{mes_ref_base}' AS mes_ref_base,\n",
    "        cpf,\n",
    "        cotista,\n",
    "        cd_ativo,\n",
    "        dt_operacao,\n",
    "        cd_tipo_operacao,\n",
    "        qt_operacao,\n",
    "        vl_preco_ativo,\n",
    "        vl_custo_total,\n",
    "        vl_rateio,\n",
    "        qt_estoque,\n",
    "        vl_pmedio\n",
    "    FROM silver.stg_controle_ativo\n",
    "    WHERE mes_referencia <= '{mes_ref_base}'\n",
    "    \"\"\"\n",
    "    \n",
    "    if cpf:\n",
    "        query += f\" AND cpf = '{cpf}'\"\n",
    "        \n",
    "    return spark.sql(query)\n",
    "\n",
    "\n",
    "def extrair_cotacoes_mes(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extrair cota√ß√µes do √∫ltimo dia do m√™s da tabela gold\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        mes_ref_base: M√™s de refer√™ncia base no formato 'YYYY-MM'\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        DATE_FORMAT(dt_pregao, 'yyyy-MM') AS mes_ref_base,\n",
    "        dt_pregao,\n",
    "        cd_ativo,\n",
    "        vl_medio AS vl_ativo\n",
    "    FROM gold.fato_cotacao t1\n",
    "    INNER JOIN gold.dim_tempo t2 ON t1.dt_pregao = t2.dt_dia\n",
    "    INNER JOIN gold.dim_ativo_financeiro t3 ON t1.sk_ativo = t3.sk_ativo\n",
    "    WHERE t1.dt_pregao  = t2.dt_ultimo_dia_util_mes \n",
    "    \"\"\"\n",
    "        \n",
    "    return spark.sql(query)\n",
    "\n",
    "\n",
    "def calcular_posicao_final(df_operacoes: DataFrame, df_cotacoes: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula a posi√ß√£o final de cada ativo no m√™s.\n",
    "    Considera m√∫ltiplas opera√ß√µes no mesmo m√™s e ajusta estoque conforme tipo de opera√ß√£o.\n",
    "    \"\"\"\n",
    "    # Ajusta a quantidade: vendas como negativas\n",
    "    df_ajustado = (\n",
    "        df_operacoes\n",
    "        .withColumn(\"qt_ajustada\",\n",
    "            F.when(F.col(\"cd_tipo_operacao\").isin(\"V\", \"VENDA\"), -F.col(\"qt_operacao\"))\n",
    "            .otherwise(F.col(\"qt_operacao\"))\n",
    "        )\n",
    "        .withColumn(\"vl_total_operacao_ajustada\",\n",
    "            (F.col(\"qt_ajustada\") * F.col(\"vl_preco_ativo\")) + F.col(\"vl_rateio\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Agrupa por ativo, cotista e m√™s de refer√™ncia\n",
    "    df_agrupado = df_ajustado.groupBy(\n",
    "        \"mes_ref_base\",\n",
    "        \"cpf\",\n",
    "        \"cotista\",\n",
    "        \"cd_ativo\"\n",
    "    ).agg(\n",
    "        F.sum(\"qt_ajustada\").alias(\"qt_estoque\"),\n",
    "        (F.sum(\"vl_total_operacao_ajustada\") / F.sum(\"qt_ajustada\")).alias(\"vl_pmedio\"),\n",
    "    )\n",
    "\n",
    "    # Join com cota√ß√µes por cd_ativo e mes_ref_base (ano_mes)\n",
    "    df_agrupado = df_agrupado.join(\n",
    "        df_cotacoes,\n",
    "        on=[\"cd_ativo\", \"mes_ref_base\"],\n",
    "        how=\"inner\" \n",
    "    )\n",
    "\n",
    "    # Calcula o valor da posi√ß√£o\n",
    "    df_resultado = (\n",
    "        df_agrupado\n",
    "        .withColumn(\"vl_investido\", F.col(\"qt_estoque\") * F.col(\"vl_pmedio\"))\n",
    "        .withColumn(\"vl_carteira\", F.col(\"qt_estoque\") * F.col(\"vl_ativo\"))\n",
    "    )\n",
    "\n",
    "    return df_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd1bf6",
   "metadata": {},
   "source": [
    "## Read Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "225bcfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identificando o per√≠odo de opera√ß√µes...\n",
      "   ‚Üí Menor data encontrada: 2024-09-23\n",
      "   ‚Üí Maior data encontrada: 2025-07-29\n",
      "   ‚Üí 11 meses para processar: 2024-09 at√© 2025-07\n"
     ]
    }
   ],
   "source": [
    "# Extrair a menor e maior data de opera√ß√£o da tabela de controle\n",
    "print(\"Identificando o per√≠odo de opera√ß√µes...\")\n",
    "df_controle = spark.table(\"silver.stg_controle_ativo\")\n",
    "\n",
    "# Buscar min e max em uma √∫nica opera√ß√£o (mais eficiente)\n",
    "min_max = df_controle.agg(\n",
    "    F.min(\"dt_operacao\").alias(\"min_date\"),\n",
    "    F.max(\"dt_operacao\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "min_date = min_max[\"min_date\"]\n",
    "max_date = min_max[\"max_date\"]\n",
    "\n",
    "print(f\"   ‚Üí Menor data encontrada: {min_date}\")\n",
    "print(f\"   ‚Üí Maior data encontrada: {max_date}\")\n",
    "\n",
    "# Gerar lista de meses a processar (desde a menor at√© a maior data)\n",
    "data_inicio = datetime.strptime(str(min_date), \"%Y-%m-%d\")\n",
    "data_fim = datetime.strptime(str(max_date), \"%Y-%m-%d\")\n",
    "\n",
    "meses_processar = []\n",
    "data_atual = data_inicio\n",
    "while data_atual <= data_fim:\n",
    "    meses_processar.append(data_atual.strftime(\"%Y-%m\"))\n",
    "    data_atual += relativedelta(months=1)\n",
    "\n",
    "print(f\"   ‚Üí {len(meses_processar)} meses para processar: {meses_processar[0]} at√© {meses_processar[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69f7ba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processando m√™s: 2024-09\n",
      "Processando m√™s: 2024-10\n",
      "Processando m√™s: 2024-11\n",
      "Processando m√™s: 2024-12\n",
      "Processando m√™s: 2025-01\n",
      "Processando m√™s: 2025-02\n",
      "Processando m√™s: 2025-03\n",
      "Processando m√™s: 2025-04\n",
      "Processando m√™s: 2025-05\n",
      "Processando m√™s: 2025-06\n",
      "Processando m√™s: 2025-07\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Lista para acumular os DataFrames de cada m√™s\n",
    "lista_posicoes = []\n",
    "\n",
    "df_cotacoes = extrair_cotacoes_mes(spark)\n",
    "\n",
    "print(f\"\\n{'='*40}\")\n",
    "\n",
    "# Loop por cada m√™s\n",
    "for mes_referencia in meses_processar:\n",
    "    print(f\"Processando m√™s: {mes_referencia}\")\n",
    "    \n",
    "    # 1. Extrair opera√ß√µes\n",
    "    df_operacoes = extrair_operacoes_mes(spark, mes_referencia)\n",
    "    count_operacoes = df_operacoes.count()\n",
    "    \n",
    "    if count_operacoes == 0:\n",
    "        print(\"‚ö† Nenhuma opera√ß√£o encontrada para o per√≠odo\")\n",
    "        continue\n",
    "    \n",
    "    # 2. Calcular posi√ß√£o final por ativo\n",
    "    df_posicao = calcular_posicao_final(df_operacoes, df_cotacoes)\n",
    "    \n",
    "    # Acumular o DataFrame na lista\n",
    "    lista_posicoes.append(df_posicao)\n",
    "\n",
    "print(f\"{'='*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9662ea8",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71db4154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Üí Total de 98 registros a serem gravados\n"
     ]
    }
   ],
   "source": [
    "# Unir todos os DataFrames de forma eficiente usando reduce\n",
    "df_union = reduce(lambda df1, df2: df1.unionByName(df2), lista_posicoes)\n",
    "    \n",
    "total_registros = df_union.count()\n",
    "print(f\"   ‚Üí Total de {total_registros} registros a serem gravados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9eddb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura das dimens√µes\n",
    "df_dim_cliente = spark.table(\"gold.dim_cliente\")\n",
    "df_dim_ativo = spark.table(\"gold.dim_ativo_financeiro\")\n",
    "\n",
    "# Join com as duas dimens√µes\n",
    "df_joined = (\n",
    "    df_union.alias(\"stg\")\n",
    "    .join(\n",
    "        df_dim_ativo.alias(\"dim_ativo\"),\n",
    "        F.col(\"stg.cd_ativo\") == F.col(\"dim_ativo.cd_ativo\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_dim_cliente.alias(\"dim_cliente\"),\n",
    "        F.col(\"stg.cpf\") == F.col(\"dim_cliente.cd_cpf_pessoa\"),\n",
    "        \"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Tratamento da chave surrogate faltante (usa -1)\n",
    "df_fato_carteira = (\n",
    "    df_joined.select(\n",
    "        F.col(\"stg.dt_pregao\").alias(\"dt_carteira\"),\n",
    "        F.when(F.col(\"sk_cliente\").isNull(), F.lit(\"-1\")).otherwise(F.col(\"sk_cliente\")).alias(\"sk_cliente\"),\n",
    "        F.when(F.col(\"sk_ativo\").isNull(), F.lit(\"-1\")).otherwise(F.col(\"sk_ativo\")).alias(\"sk_ativo\"),\n",
    "        F.col(\"qt_estoque\").alias(\"qt_ativo\"),\n",
    "        F.col(\"vl_ativo\"),\n",
    "        F.col(\"vl_pmedio\"),\n",
    "        F.col(\"vl_investido\"),\n",
    "        F.col(\"vl_carteira\"),\n",
    "        F.current_timestamp().alias(\"ts_insercao\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1c80e",
   "metadata": {},
   "source": [
    "## Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd2d70bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dynamic partition overwrite executado - apenas parti√ß√µes afetadas foram sobrescritas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_fato_carteira.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"false\") \\\n",
    "    .option(\"partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .partitionBy(\"sk_cliente\") \\\n",
    "    .save(delta_path_fato_carteira)\n",
    "\n",
    "print(\"‚úÖ Dynamic partition overwrite executado - apenas parti√ß√µes afetadas foram sobrescritas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b371b",
   "metadata": {},
   "source": [
    "## Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4c30772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jornada-financas-pessoais-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
