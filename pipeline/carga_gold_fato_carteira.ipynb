{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f8543e",
   "metadata": {},
   "source": [
    "# Carga Gold - Fato Carteira\n",
    "\n",
    "Este notebook realiza a carga da fato de cota√ß√£o (fato_cotacao) a partir dos dados da tabela staging de cota√ß√£o hist√≥rica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31477c71",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7cfa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee3a7e",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbd93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Spark 3.5.7 iniciado com Hive local persistente!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ===== CONFIGURA√á√ÉO DE DIRET√ìRIOS =====\n",
    "BASE_DIR = \"D:/Projetos/DataLake\"\n",
    "WAREHOUSE_DIR = f\"{BASE_DIR}/spark-warehouse\"\n",
    "METASTORE_DIR = f\"{BASE_DIR}/metastore_db\"\n",
    "SCRATCH_DIR = f\"{BASE_DIR}/hive_scratch\"\n",
    "\n",
    "# Criar diret√≥rios necess√°rios\n",
    "for dir_path in [WAREHOUSE_DIR, SCRATCH_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# ===== CONFIGURA√á√ÉO DO HADOOP (Windows) =====\n",
    "# Se voc√™ tem o winutils instalado, descomente e ajuste o caminho:\n",
    "# os.environ['HADOOP_HOME'] = r'C:\\hadoop'\n",
    "\n",
    "# ===== SPARK SESSION =====\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Carga fato carteira\")\n",
    "    .master(\"local[*]\")\n",
    "    # Delta Lake\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    # Hive Local (persistente)\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.sql.warehouse.dir\", f\"file:///{WAREHOUSE_DIR}\")\n",
    "    .config(\"hive.metastore.warehouse.dir\", f\"file:///{WAREHOUSE_DIR}\")\n",
    "    .config(\n",
    "        \"javax.jdo.option.ConnectionURL\",\n",
    "        f\"jdbc:derby:;databaseName={METASTORE_DIR};create=true\"\n",
    "    )\n",
    "    # Corrige problemas no Windows\n",
    "    .config(\"hive.exec.scratchdir\", SCRATCH_DIR)\n",
    "    .config(\"hive.metastore.schema.verification\", \"false\")\n",
    "    .config(\"hive.metastore.schema.verification.record.version\", \"false\")\n",
    "    .config(\"datanucleus.schema.autoCreateAll\", \"true\")\n",
    "    .enableHiveSupport()\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"\\n‚úÖ Spark {spark.version} iniciado com Hive local persistente!\")\n",
    "print(f\"üìÅ Warehouse: {WAREHOUSE_DIR}\")\n",
    "print(f\"üìÅ Metastore: {METASTORE_DIR}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d46f1",
   "metadata": {},
   "source": [
    "## Define Delta Table Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea17242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define caminhos locais onde ser√£o armazenadas as tabelas Delta\n",
    "base_silver_path = \"D:/Projetos/Jornada_financas_pessoais/data/delta/silver\"\n",
    "base_gold_path = \"D:/Projetos/Jornada_financas_pessoais/data/delta/gold\"\n",
    "\n",
    "# Define caminhos das tabelas Delta\n",
    "delta_path_controle_ativo = f\"{base_silver_path}/stg_controle_ativo\"\n",
    "delta_path_dim_ativo = f\"{base_gold_path}/dim_ativo_financeiro\"\n",
    "delta_path_fato_carteira = f\"{base_gold_path}/fato_carteira\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c603623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pipeline para carregar posi√ß√£o mensal de carteira de investimentos\n",
    "Abordagem funcional\n",
    "\"\"\"\n",
    "def extrair_operacoes_mes(spark: SparkSession, mes_ref: str, cpf: str = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extrai opera√ß√µes do m√™s da tabela silver\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        mes_ref: M√™s de refer√™ncia no formato 'YYYY-MM'\n",
    "        cpf: CPF do investidor (opcional)\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        mes_ref,\n",
    "        cpf,\n",
    "        cotista,\n",
    "        cd_ativo,\n",
    "        dt_operacao,\n",
    "        cd_tipo_operacao,\n",
    "        qt_operacao,\n",
    "        vl_preco_ativo,\n",
    "        vl_custo_total,\n",
    "        vl_liquido,\n",
    "        qt_estoque,\n",
    "        vl_pmedio,\n",
    "        vl_ganho_perda,\n",
    "        ir_mes,\n",
    "        vl_vendas_mes,\n",
    "        ts_insercao\n",
    "    FROM silver.stg_controle_ativo\n",
    "    WHERE mes_ref = '{mes_ref}'\n",
    "    \"\"\"\n",
    "    \n",
    "    if cpf:\n",
    "        query += f\" AND cpf = '{cpf}'\"\n",
    "        \n",
    "    return spark.sql(query)\n",
    "\n",
    "\n",
    "def calcular_posicao_final(df_operacoes: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula a posi√ß√£o final de cada ativo no m√™s\n",
    "    Pega a √∫ltima opera√ß√£o de cada ativo\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy(\"cpf\", \"cd_ativo\", \"mes_ref\") \\\n",
    "                        .orderBy(F.desc(\"dt_operacao\"), F.desc(\"ts_insercao\"))\n",
    "    \n",
    "    return df_operacoes \\\n",
    "        .withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "        .filter(F.col(\"rank\") == 1) \\\n",
    "        .select(\n",
    "            \"mes_ref\",\n",
    "            \"cpf\",\n",
    "            \"cotista\",\n",
    "            \"cd_ativo\",\n",
    "            \"qt_estoque\",\n",
    "            \"vl_pmedio\",\n",
    "            F.expr(\"qt_estoque * vl_pmedio\").alias(\"vl_posicao\"),\n",
    "            \"vl_ganho_perda\",\n",
    "            \"ir_mes\",\n",
    "            \"vl_vendas_mes\"\n",
    "        )\n",
    "\n",
    "\n",
    "def agregar_por_cotista(df_posicao: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega m√©tricas por cotista\n",
    "    \"\"\"\n",
    "    return df_posicao.groupBy(\"mes_ref\", \"cpf\", \"cotista\").agg(\n",
    "        F.count(\"cd_ativo\").alias(\"qt_ativos_carteira\"),\n",
    "        F.sum(\"qt_estoque\").alias(\"qt_total_acoes\"),\n",
    "        F.sum(\"vl_posicao\").alias(\"vl_total_carteira\"),\n",
    "        F.sum(\"vl_ganho_perda\").alias(\"vl_total_ganho_perda\"),\n",
    "        F.sum(\"ir_mes\").alias(\"vl_total_ir_mes\"),\n",
    "        F.sum(\"vl_vendas_mes\").alias(\"vl_total_vendas_mes\"),\n",
    "        F.avg(\"vl_pmedio\").alias(\"vl_pmedio_ponderado\")\n",
    "    )\n",
    "\n",
    "\n",
    "def calcular_metricas_financeiras(df_agregado: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adiciona m√©tricas financeiras calculadas\n",
    "    \"\"\"\n",
    "    return df_agregado \\\n",
    "        .withColumn(\n",
    "            \"pc_rentabilidade_mes\",\n",
    "            F.when(F.col(\"vl_total_carteira\") > 0,\n",
    "                   (F.col(\"vl_total_ganho_perda\") / F.col(\"vl_total_carteira\")) * 100\n",
    "            ).otherwise(0)\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"pc_ir_sobre_ganho\",\n",
    "            F.when(F.col(\"vl_total_ganho_perda\") > 0,\n",
    "                   (F.col(\"vl_total_ir_mes\") / F.col(\"vl_total_ganho_perda\")) * 100\n",
    "            ).otherwise(0)\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"vl_liquido_apos_ir\",\n",
    "            F.col(\"vl_total_ganho_perda\") - F.col(\"vl_total_ir_mes\")\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"ts_processamento\",\n",
    "            F.current_timestamp()\n",
    "        )\n",
    "\n",
    "\n",
    "def salvar_posicao_mensal(df_gold: DataFrame, modo: str = \"append\"):\n",
    "    \"\"\"\n",
    "    Salva posi√ß√£o mensal agregada na camada gold\n",
    "    \"\"\"\n",
    "    df_gold.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(modo) \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"gold.posicao_mensal_carteira\")\n",
    "    \n",
    "    print(f\"‚úì Posi√ß√£o mensal salva em gold.posicao_mensal_carteira\")\n",
    "\n",
    "\n",
    "def salvar_posicao_detalhada(df_posicao: DataFrame, modo: str = \"append\"):\n",
    "    \"\"\"\n",
    "    Salva posi√ß√£o detalhada por ativo na camada gold\n",
    "    \"\"\"\n",
    "    df_detalhada = df_posicao.withColumn(\"ts_processamento\", F.current_timestamp())\n",
    "    \n",
    "    df_detalhada.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(modo) \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"gold.posicao_detalhada_ativo\")\n",
    "    \n",
    "    print(f\"‚úì Posi√ß√£o detalhada salva em gold.posicao_detalhada_ativo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f7ba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Extraindo opera√ß√µes da silver...\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m cpf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Ou defina um CPF espec√≠fico para filtrar\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. Extraindo opera√ß√µes da silver...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m df_operacoes \u001b[38;5;241m=\u001b[39m \u001b[43mextrair_operacoes_mes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmes_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m count_operacoes \u001b[38;5;241m=\u001b[39m df_operacoes\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ‚Üí \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_operacoes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m opera√ß√µes encontradas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 39\u001b[0m, in \u001b[0;36mextrair_operacoes_mes\u001b[1;34m(spark, mes_ref, cpf)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cpf:\n\u001b[0;32m     37\u001b[0m     query \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AND cpf = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projetos\\Jornada_financas_pessoais\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Projetos\\Jornada_financas_pessoais\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Projetos\\Jornada_financas_pessoais\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
     ]
    }
   ],
   "source": [
    " # 1. Extrair opera√ß√µes\n",
    "mes_ref = \"2024-05\"\n",
    "cpf = None  # Ou defina um CPF espec√≠fico para filtrar\n",
    "\n",
    "print(\"1. Extraindo opera√ß√µes da silver...\")\n",
    "df_operacoes = extrair_operacoes_mes(spark, mes_ref, cpf)\n",
    "count_operacoes = df_operacoes.count()\n",
    "print(f\"   ‚Üí {count_operacoes} opera√ß√µes encontradas\")\n",
    "\n",
    "df_operacoes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe35b3f",
   "metadata": {},
   "source": [
    "## Read Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662bf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCESSO] Leitura da tabela staging em: D:/Projetos/Jornada_financas_pessoais/data/delta/silver/stg_cotacao_historica\n",
      "Total de registros: 5126223\n",
      "[SUCESSO] Leitura da tabela dimens√£o em: D:/Projetos/Jornada_financas_pessoais/data/delta/gold/dim_ativo_financeiro\n",
      "Total de registros: 2262\n"
     ]
    }
   ],
   "source": [
    "# L√™ a tabela staging de cota√ß√£o hist√≥rica\n",
    "df_stg_controle_ativo = spark.read.format(\"delta\").load(delta_path_controle_ativo)\n",
    "\n",
    "print(f\"[SUCESSO] Leitura da tabela staging em: {delta_path_controle_ativo}\")\n",
    "print(f\"Total de registros: {df_stg_controle_ativo.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9662ea8",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra apenas registros com tp_mercado = '10'\n",
    "df_stg_cotacao_historica = df_stg_cotacao_historica.filter(col(\"tp_mercado\") == \"010\")\n",
    "\n",
    "# Join LEFT (mant√©m todas as cota√ß√µes mesmo sem correspond√™ncia na dimens√£o)\n",
    "df_joined = (\n",
    "    df_stg_cotacao_historica.alias(\"stg\")\n",
    "    .join(\n",
    "        df_dim_ativo.alias(\"dim\"),\n",
    "        col(\"stg.cd_negociacao\") == col(\"dim.cd_ativo\"),\n",
    "        \"left\"  # mant√©m as linhas da stg mesmo se n√£o achar na dimens√£o\n",
    "    )\n",
    ")\n",
    "\n",
    "# Tratamento da chave surrogate faltante (usa -1)\n",
    "df_fato_cotacao = (\n",
    "    df_joined.select(\n",
    "        col(\"stg.dt_pregao\"),\n",
    "        when(col(\"sk_ativo\").isNull(), lit(\"-1\")).otherwise(col(\"sk_ativo\")).alias(\"sk_ativo\"),\n",
    "        col(\"vl_abertura\"),\n",
    "        col(\"vl_minimo\"),\n",
    "        col(\"vl_maximo\"),\n",
    "        col(\"vl_medio\"),\n",
    "        col(\"vl_ultimo_negocio\"),\n",
    "        col(\"qt_negocios_efetuados\").alias(\"qt_negocio\"),\n",
    "        col(\"qt_total_titulos\").alias(\"qt_titulo\"),\n",
    "        col(\"vl_total_titulos\").alias(\"vl_volume\"),\n",
    "        current_timestamp().alias(\"ts_insercao\"),\n",
    "        year(col(\"dt_pregao\")).alias(\"ano_pregao\"),\n",
    "        month(col(\"dt_pregao\")).alias(\"mes_pregao\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1c80e",
   "metadata": {},
   "source": [
    "## Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d70bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dynamic partition overwrite executado - apenas parti√ß√µes afetadas foram sobrescritas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_fato_cotacao.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"false\") \\\n",
    "    .option(\"partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .partitionBy(\"ano_pregao\", \"mes_pregao\") \\\n",
    "    .save(delta_path_fato_cotacao)\n",
    "\n",
    "print(\"‚úÖ Dynamic partition overwrite executado - apenas parti√ß√µes afetadas foram sobrescritas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b371b",
   "metadata": {},
   "source": [
    "## Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c30772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jornada-financas-pessoais-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
